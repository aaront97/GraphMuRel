name: agg_concat_min_corrected_wid
agg_type: mean
q_self_attention: True
checkpoint_option: fresh  #can be best, fresh, or resume_last
checkpoint_dir:  '/local/scratch/bat34/baseline_trained_models'
ROOT_DIR: /auto/homes/bat34/VQA_PartII/models/baseline/
bottom_up_features_dir: /local/scratch/bat34/2018-04-27_bottom-up-attention_fixed_36/
skipthoughts_dir: '/auto/homes/bat34/VQA_PartII/data/skipthoughts'
processed_dir: '/auto/homes/bat34/VQA_PartII/data/processed_splits/'
include_keys: ['seed',
               'agg_type',
               'q_self_attention',
               'txt_enc',
               'batch_size',
               'lr',
               'fusion_type',
               ]
vqa_dir: '/auto/homes/bat34/VQA'
dropout: 0.2
txt_enc: BayesianUniSkip
batch_size: 256
loss_function: 'NLLLoss' # can be soft_cross_entropy or NLLLoss, remember to change RESULTS_FILE_PATH
RESULTS_FILE_PATH: '/auto/homes/bat34/VQA_PartII/models/baseline/{}-temp.json'
grad_clip: 0.25
lr: 0.0003
epochs: 25
checkpoint_every: 1
reduction_factor: 8
num_workers: 4
seed: 2048
log_every: 50
gradual_warmup_steps: [1.0, 4.0, 7.0] #torch.linspace
lr_decay_epochs: [14, 24, 2] #range
lr_decay_rate: .25
fusion_type: concat_mlp
fusion_block:
    type: block
    input_dims: [2400, 2048]
    output_dims: 3000
    dropout_prelin: 0.
    dropout_input: 0.1
    chunks: 20
    rank: 15
    mm_dim: 1000
fusion_mlp:
    input_dims: [4800, 2048]
    out_dim: 3000
    dropout: 0.2
    hidden_list: [3200, 3200]
q_att:
    q_linear0:
        input_dim: 2400
        output_dim: 512
    q_linear1:
        input_dim: 512
        output_dim: 2
