name: concat_baseline_self_attention_corrected_wid
ROOT_DIR: /auto/homes/bat34/VQA_PartII/models/baseline/
checkpoint_dir: /local/scratch/bat34/baseline_trained_models
bottom_up_features_dir: /auto/homes/bat34/2018-04-27_bottom-up-attention_fixed_36/
skipthoughts_dir: '/auto/homes/bat34/VQA_PartII/data/skipthoughts'
processed_dir: '/auto/homes/bat34/VQA_PartII/data/processed_splits/'
vqa_dir: '/auto/homes/bat34/VQA'
checkpoint_option: best #can be best, fresh, or resume_last
include_keys: ['seed',
               'txt_enc',
               'batch_size',
               'lr',
               'lr_decay_rate',
               'hidden_list',
               'attention_fusion_type',
               'final_fusion_type']
dropout: 0.2
txt_enc: BayesianUniSkip
seed: 2048
batch_size: 256
loss_function: 'NLLLoss' # can be soft_cross_entropy or NLLLoss, remember to change RESULTS_FILE_PATH
RESULTS_FILE_PATH: '/auto/homes/bat34/VQA_PartII/models/baseline/{}-temp.json'
grad_clip: 0.25
lr: 0.0003
epochs: 25
checkpoint_every: 1
reduction_factor: 8
num_workers: 4
log_every: 50
gradual_warmup_steps: [1.0, 4.0, 7.0] #torch.linspace
lr_decay_epochs: [14, 24, 2] #range
lr_decay_rate: .25
hidden_list: [1600, 1600]
attention_fusion_type: block #block or concat_mlp
attention_fusion_block:
    type: block
    input_dims: [4800, 2048]
    output_dims: 1000
    dropout_prelin: 0.
    dropout_input: 0.1
    chunks: 20
    rank: 15
    mm_dim: 1000
attention_fusion_mlp:
    input_dims: [4800, 2048]
    out_dim: 1000
    dropout: 0.2
    hidden_list: [1600, 1600]
final_fusion_type: block
final_fusion_block:
    type: block
    input_dims: [4800, 4096]
    output_dims: 3000
    dropout_prelin: 0.1
    dropout_input: 0.
    chunks: 20
    rank: 10
    mm_dim: 1000
final_fusion_mlp:
    input_dims: [4800, 4096]
    out_dim: 3000
    dropout: 0.2
    hidden_list: [1600, 1600]
obj_att:
    obj_linear0:
        input_dim: 1000
        output_dim: 512
    obj_linear1:
        input_dim: 512
        output_dim: 2
q_att:
    q_linear0:
        input_dim: 2400
        output_dim: 512
    q_linear1:
        input_dim: 512
        output_dim: 2
